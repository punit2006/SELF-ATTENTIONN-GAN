# -*- coding: utf-8 -*-
"""SELF-ATTENTIONN-GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bKwvxt9WDf24eERPtgxQEHdEFjPj6C54?usp=sharing
"""

!pip install torch torchvision matplotlib numpy pandas tqdm wandb

# Import core libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm
import wandb  # For experiment tracking

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Self-Attention Layer ---
class SelfAttention(nn.Module):
    def __init__(self, in_channels):
        super(SelfAttention, self).__init__()
        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, C, height, width = x.size()
        q = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)
        k = self.key(x).view(batch_size, -1, height * width)
        v = self.value(x).view(batch_size, -1, height * width)

        attention = torch.bmm(q, k)
        attention = torch.softmax(attention, dim=-1)

        out = torch.bmm(v, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, height, width)
        return self.gamma * out + x

class Generator(nn.Module):
    def __init__(self, latent_dim=100, img_channels=3, feature_map=64):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            # Input: [batch_size, latent_dim, 1, 1]
            # Output: [batch_size, feature_map * 8, 4, 4]
            nn.ConvTranspose2d(latent_dim, feature_map * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(feature_map * 8),
            nn.ReLU(True),

            # Add self-attention
            SelfAttention(feature_map * 8),

            # Output: [batch_size, feature_map * 4, 8, 8]
            nn.ConvTranspose2d(feature_map * 8, feature_map * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_map * 4),
            nn.ReLU(True),

            # Output: [batch_size, feature_map * 2, 16, 16]
            nn.ConvTranspose2d(feature_map * 4, feature_map * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_map * 2),
            nn.ReLU(True),

            # Output: [batch_size, feature_map, 32, 32]
            nn.ConvTranspose2d(feature_map * 2, feature_map, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_map),
            nn.ReLU(True),

            # Output: [batch_size, img_channels, 64, 64]
            nn.ConvTranspose2d(feature_map, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()  # Output in range [-1, 1]
        )

    def forward(self, x):
        return self.main(x)

class Discriminator(nn.Module):
    def __init__(self, img_channels=3, feature_map=64):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # Input: [batch_size, img_channels, 64, 64]
            # Output: [batch_size, feature_map, 32, 32]
            nn.Conv2d(img_channels, feature_map, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # Add self-attention
            SelfAttention(feature_map),

            # Output: [batch_size, feature_map * 2, 16, 16]
            nn.Conv2d(feature_map, feature_map * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_map * 2),
            nn.LeakyReLU(0.2, inplace=True),

            # Output: [batch_size, feature_map * 4, 8, 8]
            nn.Conv2d(feature_map * 2, feature_map * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_map * 4),
            nn.LeakyReLU(0.2, inplace=True),

            # Output: [batch_size, 1, 4, 4]
            nn.Conv2d(feature_map * 4, 1, 4, 1, 0, bias=False),
        )

    def forward(self, x):
        x = self.main(x)
        x = torch.mean(x, dim=[2, 3])  # Global average pooling
        return torch.sigmoid(x).view(-1, 1)

# --- Initialize Models ---
latent_dim = 100
generator = Generator(latent_dim).to(device)
discriminator = Discriminator().to(device)

# --- Loss and Optimizers ---
criterion = nn.BCELoss()
optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

def train_gan(dataloader, epochs=50):
    for epoch in range(epochs):
        for i, (real_images, _) in enumerate(tqdm(dataloader)):
            # Move images to device
            real_images = real_images.to(device)
            batch_size = real_images.size(0)

            # Train Discriminator
            optimizer_D.zero_grad()
            real_labels = torch.ones(batch_size, 1, device=device)
            fake_labels = torch.zeros(batch_size, 1, device=device)

            # Forward pass real images
            outputs = torch.sigmoid(discriminator(real_images))
            d_loss_real = criterion(outputs, real_labels)

            # Generate fake images
            noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)
            fake_images = generator(noise)
            outputs = torch.sigmoid(discriminator(fake_images.detach()))
            d_loss_fake = criterion(outputs, fake_labels)

            # Total discriminator loss
            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            optimizer_D.step()

            # Train Generator
            optimizer_G.zero_grad()
            outputs = torch.sigmoid(discriminator(fake_images))
            g_loss = criterion(outputs, real_labels)
            g_loss.backward()
            optimizer_G.step()

        # Log to wandb
        wandb.log({"Generator Loss": g_loss.item(), "Discriminator Loss": d_loss.item()})

# --- Load Data ---
transform = transforms.Compose([
    transforms.Resize(64),
    transforms.CenterCrop(64),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

dataset = datasets.CIFAR10(root='./data', download=True, transform=transform)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

# --- Initialize wandb ---
wandb.init(project="attention-gan")

# --- Train ---
train_gan(dataloader, epochs=50)

print(type(generator))

import torchvision.utils as vutils
import matplotlib.pyplot as plt
import os

# Folder for generated images
os.makedirs("generated_samples", exist_ok=True)

# Number of images
num_images = 16

# Sample random noise
noise = torch.randn(num_images, latent_dim, 1, 1, device=device)

# Generate
with torch.no_grad():
    fake_images = generator(noise).cpu()

# Save as grid
vutils.save_image(fake_images, "generated_samples/fake_samples.png", normalize=True, nrow=4)

# Show inline
plt.figure(figsize=(8,8))
plt.axis("off")
plt.title("Generated Images")
plt.imshow(np.transpose(vutils.make_grid(fake_images, padding=2, normalize=True), (1,2,0)))
plt.show()

# Save generator and discriminator
torch.save(generator.state_dict(), "generator.pth")
torch.save(discriminator.state_dict(), "discriminator.pth")

print("✅ Models saved successfully!")

# Recreate model architecture
generator = Generator(latent_dim).to(device)
discriminator = Discriminator().to(device)

# Load weights
generator.load_state_dict(torch.load("generator.pth", map_location=device))
discriminator.load_state_dict(torch.load("discriminator.pth", map_location=device))

generator.eval()
discriminator.eval()

print("✅ Models loaded successfully!")

import torchvision.utils as vutils
import matplotlib.pyplot as plt
import os

os.makedirs("generated_samples", exist_ok=True)

num_images = 16
noise = torch.randn(num_images, latent_dim, 1, 1, device=device)

with torch.no_grad():
    fake_images = generator(noise).cpu()

vutils.save_image(fake_images, "generated_samples/fake_samples.png", normalize=True, nrow=4)

plt.figure(figsize=(8,8))
plt.axis("off")
plt.title("Generated Images")
plt.imshow(np.transpose(vutils.make_grid(fake_images, padding=2, normalize=True), (1,2,0)))
plt.show()

